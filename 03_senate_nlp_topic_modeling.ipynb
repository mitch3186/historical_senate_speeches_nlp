{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Senate NLP Project - Topic Modeling\n",
    "### By: Mitch Brinkman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from sklearn.feature_extraction import text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senate_func import build_edu_era_tables\n",
    "from senate_func import build_hc_era_tables\n",
    "from senate_func import build_fin_era_tables\n",
    "from senate_func import clean_senate_speech\n",
    "from senate_func import prez\n",
    "from senate_func import display_topics\n",
    "from senate_func import drop_columns\n",
    "from senate_func import NLPProcessor\n",
    "# from senate_func import find_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DF Types & Names:\n",
    "    Topic\n",
    "        Education: edu_df\n",
    "        Healthcare: hc_df\n",
    "        Banking: fin_df\n",
    "    Era\n",
    "        1980-1988: (edu/hc/fin)_reagan_df\n",
    "        1989-1992: (edu/hc/fin)_bush_df\n",
    "        1993-2000: (edu/hc/fin)_clinton_df\n",
    "        2001-2008: (edu/hc/fin)_w_bush_df\n",
    "        2009-2016: (edu/hc/fin)_obama_df\n",
    "    Gender\n",
    "        female_df\n",
    "        male_df\n",
    "        edu_female_df\n",
    "        hc_female_df\n",
    "        fin_female_df\n",
    "        edu_male_df\n",
    "        hc_male_df\n",
    "        fin_male_df \n",
    "    Party\n",
    "        edu_dem_df\n",
    "        hc_dem_df\n",
    "        fin_dem_df\n",
    "        edu_rep_df\n",
    "        hc_rep_df\n",
    "        fin_rep_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds and pickles era specific tables needed into the appropriate folder for use at any time\n",
    "build_fin_era_tables(prez)\n",
    "build_edu_era_tables(prez)\n",
    "build_hc_era_tables(prez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Era - split into Themes (Edu, HC or Fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_reagan_df = pd.read_pickle('./data/pickles/era/edu_reagan_df.pkl')\n",
    "hc_reagan_df = pd.read_pickle('./data/pickles/era/hc_reagan_df.pkl')\n",
    "fin_reagan_df = pd.read_pickle('./data/pickles/era/fin_reagan_df.pkl')\n",
    "edu_bush_df = pd.read_pickle('./data/pickles/era/edu_bush_df.pkl')\n",
    "hc_bush_df = pd.read_pickle('./data/pickles/era/hc_bush_df.pkl')\n",
    "fin_bush_df = pd.read_pickle('./data/pickles/era/fin_bush_df.pkl')\n",
    "edu_clinton_df = pd.read_pickle('./data/pickles/era/edu_clinton_df.pkl')\n",
    "hc_clinton_df = pd.read_pickle('./data/pickles/era/hc_clinton_df.pkl')\n",
    "fin_clinton_df = pd.read_pickle('./data/pickles/era/fin_clinton_df.pkl')\n",
    "edu_w_bush_df = pd.read_pickle('./data/pickles/era/edu_w_bush_df.pkl')\n",
    "hc_w_bush_df = pd.read_pickle('./data/pickles/era/hc_w_bush_df.pkl')\n",
    "fin_w_bush_df = pd.read_pickle('./data/pickles/era/fin_w_bush_df.pkl')\n",
    "edu_obama_df = pd.read_pickle('./data/pickles/era/edu_obama_df.pkl')\n",
    "hc_obama_df = pd.read_pickle('./data/pickles/era/hc_obama_df.pkl')\n",
    "fin_obama_df = pd.read_pickle('./data/pickles/era/fin_obama_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edu_df = pd.read_pickle('./data/pickles/topic/edu_df.pkl')\n",
    "# hc_df = pd.read_pickle('./data/pickles/topic/hc_df.pkl')\n",
    "fin_df = pd.read_pickle('./data/pickles/topic/fin_df.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Democrats by 80s, 90s and 00s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_dem_00_df = fin_df[(fin_df['date'] > '2000-01-01') & (fin_df['party'] == 'D')]\n",
    "fin_dem_90_df = fin_df[(fin_df['date'] > '1990-01-01') & (fin_df['date'] < '2000-01-01') & (fin_df['party'] == 'D')]\n",
    "fin_dem_80_df = fin_df[(fin_df['date'] < '1990-01-01') & (fin_df['party'] == 'D')]\n",
    "hc_dem_00_df = hc_df[(hc_df['date'] > '2000-01-01') & (hc_df['party'] == 'D')]\n",
    "hc_dem_90_df = hc_df[(hc_df['date'] > '1990-01-01') & (hc_df['date'] < '2000-01-01') & (hc_df['party'] == 'D')]\n",
    "hc_dem_80_df = hc_df[(hc_df['date'] < '1990-01-01') & (hc_df['party'] == 'D')]\n",
    "edu_dem_00_df = edu_df[(edu_df['date'] > '2000-01-01') & (edu_df['party'] == 'D')]\n",
    "edu_dem_90_df = edu_df[(edu_df['date'] > '1990-01-01') & (edu_df['date'] < '2000-01-01') & (edu_df['party'] == 'D')]\n",
    "edu_dem_80_df = edu_df[(edu_df['date'] < '1990-01-01') & (edu_df['party'] == 'D')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Republicans by 80s, 90s and 00s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_00_df = fin_df[(fin_df['date'] > '2000-01-01') & (fin_df['party'] == 'D')]\n",
    "fin_rep_90_df = fin_df[(fin_df['date'] > '1990-01-01') & (fin_df['date'] < '2000-01-01') & (fin_df['party'] == 'D')]\n",
    "fin_rep_80_df = fin_df[(fin_df['date'] < '1990-01-01') & (fin_df['party'] == 'D')]\n",
    "hc_rep_00_df = hc_df[(hc_df['date'] > '2000-01-01') & (hc_df['party'] == 'D')]\n",
    "hc_rep_90_df = hc_df[(hc_df['date'] > '1990-01-01') & (hc_df['date'] < '2000-01-01') & (hc_df['party'] == 'D')]\n",
    "hc_rep_80_df = hc_df[(hc_df['date'] < '1990-01-01') & (hc_df['party'] == 'D')]\n",
    "edu_rep_00_df = edu_df[(edu_df['date'] > '2000-01-01') & (edu_df['party'] == 'D')]\n",
    "edu_rep_90_df = edu_df[(edu_df['date'] > '1990-01-01') & (edu_df['date'] < '2000-01-01') & (edu_df['party'] == 'D')]\n",
    "edu_rep_80_df = edu_df[(edu_df['date'] < '1990-01-01') & (edu_df['party'] == 'D')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_hc_clinton_df = hc_clinton_df[(hc_clinton_df['date'] < '1995-01-01')]\n",
    "second_hc_clinton_df = hc_clinton_df[(hc_clinton_df['date'] > '1995-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_dem_80_df = fin_df[(fin_df['date'] < '1990-01-01') & (fin_df['party'] == 'D')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM & Topic Modeling Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Each set of topics is then loaded into a word document to be interpreted in an easier format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "bank, wa, year, capital, loss, mr, failure, insurance\n",
      "\n",
      "Topic  2\n",
      "bank, farmer, agricultural, farm, loan, debt, problem, agricultural bank\n",
      "\n",
      "Topic  3\n",
      "bank, federal, agency, regulatory, banking, federal reserve, reserve, ha\n",
      "\n",
      "Topic  4\n",
      "loan, student, student loan, program, bond, cost, state, act\n",
      "\n",
      "Topic  5\n",
      "security, bank, commercial, market, firm, financial, glasssteagall, underwriting\n",
      "\n",
      "Topic  6\n",
      "loan, farmer, market, ha, program, year, export, capital\n",
      "\n",
      "Topic  7\n",
      "bank, title, insurance, legislation, think, moratorium, ii, problem\n",
      "\n",
      "Topic  8\n",
      "bank, company, holding, holding company, banking, bank holding, state, act\n",
      "\n",
      "Topic  9\n",
      "business, bank, country, american, ha, local, state, banking\n",
      "\n",
      "Topic  10\n",
      "banking, committee, state, new, financial, nonbank, ha, nonbank bank\n",
      "\n",
      "Topic  11\n",
      "reserve, bank, federal, money, federal reserve, tax, deposit, billion\n",
      "\n",
      "Topic  12\n",
      "bank, international, financial, banking, country, world, currency, foreign\n"
     ]
    }
   ],
   "source": [
    "# Pipeline processor with 1-2 ngrams, stop-words, lemmatizing\n",
    "\n",
    "nlp = NLPProcessor(CountVectorizer(stop_words='english',ngram_range=(1, 2),max_df=.97,min_df=.05)\n",
    "                   ,TreebankWordTokenizer().tokenize,\n",
    "                   clean_senate_speech, WordNetLemmatizer().lemmatize)\n",
    "\n",
    "nlp.fit(fin_dem_80_df['speech'])\n",
    "fin_dem_80_dtm = nlp.transform(fin_dem_80_df['speech'])\n",
    "fin_dem_80_cv = nlp.vectorizer\n",
    "nmf_model = NMF(12)\n",
    "doc_topic = nmf_model.fit_transform(fin_dem_80_dtm)\n",
    "display_topics(nmf_model, fin_dem_80_cv.get_feature_names(), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Archive- DISREGARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_edu_era_tables (prez_list):\n",
    "#     for i in prez_list:\n",
    "#             i_df = pd.read_pickle('./data/pickles/era/'+i+'_df.pkl')\n",
    "#             edu_i_df = i_df.loc[(i_df['labels']=='Education')]\n",
    "#             pickle.dump(edu_i_df, open(\"./data/pickles/era/edu_\"+i+\"_df.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_hc_era_tables (prez_list):\n",
    "#     for i in prez_list:\n",
    "#             i_df = pd.read_pickle('./data/pickles/era/'+i+'_df.pkl')\n",
    "#             hc_i_df = i_df.loc[(i_df['labels']=='Healthcare')]\n",
    "#             pickle.dump(hc_i_df, open(\"./data/pickles/era/hc_\"+i+\"_df.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_fin_era_tables (prez_list):\n",
    "#     for i in prez_list:\n",
    "#             i_df = pd.read_pickle('./data/pickles/era/'+i+'_df.pkl')\n",
    "#             fin_i_df = i_df.loc[(i_df['labels']=='Financial')]\n",
    "#             pickle.dump(fin_i_df, open(\"./data/pickles/era/fin_\"+i+\"_df.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_senate_speech(text):\n",
    "#     '''Make text lowercase, remove text in square brackets, \n",
    "#     remove punctuation and remove words containing numbers.\n",
    "#     '''\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\n",
    "#     text = text.lower()\n",
    "#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "#     return text\n",
    "\n",
    "# big_wash = lambda x: clean_senate_speech(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "#     \"\"\"\n",
    "#     Displays the top n terms in each topic\n",
    "#     \"\"\"\n",
    "#     for ix, topic in enumerate(model.components_):\n",
    "#         if not topic_names or not topic_names[ix]:\n",
    "#             print(\"\\nTopic \", ix + 1)\n",
    "#         else:\n",
    "#             print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "#         print(\", \".join([feature_names[i]\n",
    "#                         for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NLPProcessor:\n",
    "    \n",
    "#     def __init__(self, vectorizer_class, tokenizer_function, cleaning_function,lemmer_function):\n",
    "#         self.vectorizer = vectorizer_class\n",
    "#         self.tokenizer = tokenizer_function\n",
    "#         self.cleaning_function = cleaning_function\n",
    "#         self.lemmer = lemmer_function\n",
    "    \n",
    "#     def fit(self, corpus_list_to_fit):\n",
    "#         cleaned_corpus = list(map(self.cleaning_function, corpus_list_to_fit))\n",
    "# #         print(cleaned_corpus)\n",
    "#         tokenized_list = list(map(self.tokenizer, cleaned_corpus))\n",
    "# #         print(tokenized_list)\n",
    "#         lemmed_list = [' '.join(list(map(self.lemmer, item))) for item in tokenized_list]\n",
    "# #         print(lemmed_list)\n",
    "#         return self.vectorizer.fit(lemmed_list)\n",
    "    \n",
    "#     def transform(self, corpus_list_to_clean):\n",
    "#         cleaned_corpus = list(map(self.cleaning_function, corpus_list_to_clean))\n",
    "#         tokenized_list = list(map(self.tokenizer, cleaned_corpus))\n",
    "#         lemmed_list = [' '.join(list(map(self.lemmer, item))) for item in tokenized_list]\n",
    "#         return pd.DataFrame(self.vectorizer.transform(lemmed_list).toarray(), \n",
    "#                             columns=self.vectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
